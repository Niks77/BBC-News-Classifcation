{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8b1b1746",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/nik/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/nik/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk \n",
    "from nltk.tokenize import word_tokenize  \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "from string import punctuation\n",
    "from math import log10\n",
    "stop_words = stopwords.words('english')\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9119c43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(df, size):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    X = df.copy()\n",
    "    target = df['Category']\n",
    "    X = X.drop('Category',axis = 1)\n",
    "    train_X, test_X, train_y, test_y = train_test_split(X, target, test_size=size, stratify=target)\n",
    "    train_X['Category'] = train_y\n",
    "    test_X['Category'] = test_y\n",
    "    tf = dict()\n",
    "    for i in df[\"Category\"].unique():\n",
    "        tf[i] = {}\n",
    "    cf = dict()\n",
    "    result = {row.ArticleId: [row.Text, row.Category] for (index, row) in train_X.iterrows()}\n",
    "    for i in result:\n",
    "    #     print(i)\n",
    "        result[i][0] = result[i][0].lower()\n",
    "        result[i][0] = word_tokenize(result[i][0])\n",
    "        no_stop_words = []                   \n",
    "        for word in result[i][0]:                                   #iterating through every token.\n",
    "            if word not in stop_words:                        \n",
    "                no_stop_words.append(word)              #if the word is not a stop word append it.\n",
    "        result[i][0] = no_stop_words\n",
    "        no_punc = []\n",
    "        for word in result[i][0]:                                   #iterating through every token.\n",
    "            if word not in punctuation:\n",
    "                for punctuationChar in punctuation :\n",
    "                    word = word.replace(punctuationChar,'')\n",
    "                no_punc.append(word)              #if the word is not a stop word append it.\n",
    "        result[i][0] = no_punc\n",
    "\n",
    "        word_list = result[i][0]\n",
    "        res_list = []\n",
    "        for word in word_list:\n",
    "            res_list.append( lemmatizer.lemmatize(word))\n",
    "        result[i][0] = res_list\n",
    "    for i in result:\n",
    "        word_list = result[i][0]\n",
    "        target = result[i][1]\n",
    "        for word in word_list:\n",
    "            if word in tf[target].keys():\n",
    "                tf[target][word] = tf[target][word] + 1\n",
    "            else:\n",
    "                tf[target][word] = 1\n",
    "    maxi = 0\n",
    "    for i in result:\n",
    "        word_list = result[i][0]\n",
    "        for word in word_list:\n",
    "            count = 0\n",
    "            for c in tf.keys():\n",
    "                if word in tf[c].keys():\n",
    "                    count = count + 1\n",
    "            maxi = max(count,maxi)\n",
    "            cf[word] = count\n",
    "\n",
    "    tf_icf = tf.copy()\n",
    "    for c in cf:\n",
    "        for i in tf.keys():\n",
    "            if c in tf[i].keys() and cf[c] != 0:\n",
    "                tf_icf[i][c] = tf[i][c]*log10(5.0/cf[c])\n",
    "    word_corpus = dict()\n",
    "    for key in result.keys():\n",
    "        for word in result[key][0]:\n",
    "            word_corpus[word]=1\n",
    "    conditionProb = {}\n",
    "    for key in tf_icf.keys():\n",
    "        conditionProb[key] = {}\n",
    "    for c in cf:\n",
    "        for key in tf_icf.keys():\n",
    "            if key in tf_icf.keys() and c in tf_icf[key]:\n",
    "                conditionProb[key][c] = (tf_icf[key][c] + 1)/(sum(tf_icf[key].values()) + len(word_corpus))\n",
    "#                 conditionProb[key][c] = (tf_icf[key][c])/(sum(tf_icf[key].values()))\n",
    "    \n",
    "    total_sum = 0\n",
    "    \n",
    "    for i in tf.keys():\n",
    "        count = 0\n",
    "        prior_prob[i] = 0\n",
    "        for c in result:\n",
    "            target = result[c][1]\n",
    "            if i == target:\n",
    "                prior_prob[i] += 1\n",
    "                total_sum += 1\n",
    "    \n",
    "    for i in tf.keys():\n",
    "        prior_prob[i] = prior_prob[i]/total_sum\n",
    "    mapping = dict()\n",
    "    count = 0;\n",
    "    for i in tf.keys():\n",
    "        mapping[i] = count\n",
    "        count = count + 1\n",
    "    return test_X, conditionProb,prior_prob,mapping\n",
    "    \n",
    "    \n",
    "    \n",
    "def trainIDF(df, size):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    X = df.copy()\n",
    "    target = df['Category']\n",
    "    X = X.drop('Category',axis = 1)\n",
    "    train_X, test_X, train_y, test_y = train_test_split(X, target, test_size=size, stratify=target)\n",
    "    train_X['Category'] = train_y\n",
    "    test_X['Category'] = test_y\n",
    "    tf = dict()\n",
    "    for i in df[\"Category\"].unique():\n",
    "        tf[i] = {}\n",
    "    df = dict()\n",
    "    result = {row.ArticleId: [row.Text, row.Category] for (index, row) in train_X.iterrows()}\n",
    "    for i in result:\n",
    "    #     print(i)\n",
    "        result[i][0] = result[i][0].lower()\n",
    "        result[i][0] = word_tokenize(result[i][0])\n",
    "        no_stop_words = []                   \n",
    "        for word in result[i][0]:                                   #iterating through every token.\n",
    "            if word not in stop_words:                        \n",
    "                no_stop_words.append(word)              #if the word is not a stop word append it.\n",
    "        result[i][0] = no_stop_words\n",
    "        no_punc = []\n",
    "        for word in result[i][0]:                                   #iterating through every token.\n",
    "            if word not in punctuation:\n",
    "                for punctuationChar in punctuation :\n",
    "                    word = word.replace(punctuationChar,'')\n",
    "                no_punc.append(word)              #if the word is not a stop word append it.\n",
    "        result[i][0] = no_punc\n",
    "\n",
    "        word_list = result[i][0]\n",
    "        res_list = []\n",
    "        for word in word_list:\n",
    "            res_list.append( lemmatizer.lemmatize(word))\n",
    "        result[i][0] = res_list\n",
    "    for i in result:\n",
    "        word_list = result[i][0]\n",
    "        target = result[i][1]\n",
    "        for word in word_list:\n",
    "            if word in tf[target].keys():\n",
    "                tf[target][word] = tf[target][word] + 1\n",
    "            else:\n",
    "                tf[target][word] = 1\n",
    "    maxi = 0\n",
    "    df = dict()\n",
    "    maxi = 0\n",
    "    for i in result:\n",
    "        word_list = result[i][0]\n",
    "        for word in word_list:\n",
    "            if word in df.keys():\n",
    "                continue\n",
    "            count = 0\n",
    "            for c in result.keys():\n",
    "                if word in result[c][0]:\n",
    "                    count = count + 1\n",
    "            maxi = max(count,maxi)\n",
    "            df[word] = count\n",
    "    \n",
    "    word_corpus = dict()\n",
    "    for key in result.keys():\n",
    "        for word in result[key][0]:\n",
    "            word_corpus[word]=1\n",
    "    conditionProb = {}\n",
    "    tf_idf = tf.copy()\n",
    "    for key in tf_idf.keys():\n",
    "        conditionProb[key] = {}\n",
    " \n",
    "    for c in df:\n",
    "        for i in tf.keys():\n",
    "            if c in tf[i].keys() and df[c] != 0:\n",
    "                tf_idf[i][c] = tf[i][c]*log10(len(X)/df[c])\n",
    "    for c in df:\n",
    "        for key in tf_idf.keys():\n",
    "            if key in tf_idf.keys() and c in tf_idf[key]:\n",
    "                conditionProb[key][c] = (tf_idf[key][c] + 1)/(sum(tf_idf[key].values()) + len(word_corpus))\n",
    "    prior_prob = {}\n",
    "    \n",
    "    total_sum = 0\n",
    "    \n",
    "    for i in tf.keys():\n",
    "        count = 0\n",
    "        prior_prob[i] = 0\n",
    "        for c in result:\n",
    "            target = result[c][1]\n",
    "            if i == target:\n",
    "                prior_prob[i] += 1\n",
    "                total_sum += 1\n",
    "    \n",
    "    for i in tf.keys():\n",
    "        prior_prob[i] = prior_prob[i]/total_sum\n",
    "    mapping = dict()\n",
    "    count = 0;\n",
    "    for i in tf.keys():\n",
    "        mapping[i] = count\n",
    "        count = count + 1\n",
    "    return test_X, conditionProb,prior_prob,mapping\n",
    "    \n",
    "    \n",
    "\n",
    "def predict(test_x,conditionProb,prior_prob,mapping):\n",
    "    result1 = {row.ArticleId: [row.Text, row.Category] for (index, row) in test_X.iterrows()}\n",
    "    for i in result1:\n",
    "    #     print(i)\n",
    "        result1[i][0] = result1[i][0].lower()\n",
    "        result1[i][0] = word_tokenize(result1[i][0])\n",
    "        no_stop_words = []                   \n",
    "        for word in result1[i][0]:                                   #iterating through every token.\n",
    "            if word not in stop_words:                        \n",
    "                no_stop_words.append(word)              #if the word is not a stop word append it.\n",
    "        result1[i][0] = no_stop_words\n",
    "        no_punc = []\n",
    "        for word in result1[i][0]:                                   #iterating through every token.\n",
    "            if word not in punctuation:\n",
    "                for punctuationChar in punctuation :\n",
    "                    word = word.replace(punctuationChar,'')\n",
    "                no_punc.append(word)              #if the word is not a stop word append it.\n",
    "        result1[i][0] = no_punc\n",
    "    for i in result1:\n",
    "        word_list = result1[i][0]\n",
    "        res_list = []\n",
    "        for word in word_list:\n",
    "            res_list.append( lemmatizer.lemmatize(word))\n",
    "    result1[i][0] = res_list\n",
    "\n",
    "    rows, cols = (5, 5)\n",
    "    confusionMatrix = [ [ 0 for i in range(5) ] for j in range(5) ]\n",
    "    \n",
    "\n",
    "    count = 0\n",
    "    for c in result1.keys():\n",
    "        mini = 0\n",
    "        pred = None\n",
    "        for key in conditionProb.keys():\n",
    "            y = 0\n",
    "            for word in result1[c][0]:\n",
    "                if word in conditionProb[key].keys():\n",
    "                    y += log10(conditionProb[key][word])\n",
    "\n",
    "                y += log10(prior_prob[key])\n",
    "                if(y < mini):\n",
    "                    mini = y\n",
    "                    pred = key\n",
    "        if pred == result1[c][1]:\n",
    "            count = count + 1\n",
    "\n",
    "        predicted = mapping[pred]\n",
    "        actual = mapping[result1[c][1]]\n",
    "        confusionMatrix[predicted][actual] += 1\n",
    "\n",
    "\n",
    "    print(\"accuracy\",count / len(result1.keys()))\n",
    "    print(\"confusion matrix\",confusionMatrix)\n",
    "    num_classes = len(confusionMatrix)\n",
    "\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    for i in range(num_classes):\n",
    "        tp = confusionMatrix[i][i]\n",
    "        fp = sum([confusionMatrix[j][i] for j in range(num_classes) if j != i])\n",
    "        fn = sum([confusionMatrix[i][j] for j in range(num_classes) if j != i])\n",
    "\n",
    "        precision_i = tp / (tp + fp)\n",
    "        recall_i = tp / (tp + fn)\n",
    "        f1_i = 2 * (precision_i * recall_i) / (recall_i + precision_i)\n",
    "\n",
    "        precision.append(precision_i)\n",
    "        f1.append(f1_i)\n",
    "        recall.append(recall_i)\n",
    "    print(\"confusionMatrix\",confusionMatrix)\n",
    "    avg_precision = sum(precision) / num_classes\n",
    "    avg_recall = sum(recall) / num_classes\n",
    "    avg_f1 = sum(f1) / num_classes\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall: \", recall)\n",
    "    print(\"F1: \",f1)\n",
    "    print(\"Average Precision: \", avg_precision)\n",
    "    print(\"Average Recall: \", avg_recall)\n",
    "    print(\"Average F1: \",avg_f1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eebc8ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ArticleId                                               Text  Category\n",
      "0       1833  worldcom ex-boss launches defence lawyers defe...  business\n",
      "1        154  german business confidence slides german busin...  business\n",
      "2       1101  bbc poll indicates economic gloom citizens in ...  business\n",
      "3       1976  lifestyle  governs mobile choice  faster  bett...      tech\n",
      "4        917  enron bosses in $168m payout eighteen former e...  business\n",
      "\n",
      "TF-ICF 20% test\n",
      "accuracy 0.9748322147651006\n",
      "confusion matrix [[125, 0, 0, 0, 0], [8, 103, 1, 0, 2], [2, 0, 109, 0, 0], [0, 0, 0, 137, 0], [0, 1, 0, 1, 107]]\n",
      "confusionMatrix [[125, 0, 0, 0, 0], [8, 103, 1, 0, 2], [2, 0, 109, 0, 0], [0, 0, 0, 137, 0], [0, 1, 0, 1, 107]]\n",
      "Precision:  [0.9259259259259259, 0.9903846153846154, 0.990909090909091, 0.9927536231884058, 0.981651376146789]\n",
      "Recall:  [1.0, 0.9035087719298246, 0.9819819819819819, 1.0, 0.981651376146789]\n",
      "F1:  [0.9615384615384615, 0.9449541284403671, 0.9864253393665158, 0.9963636363636363, 0.981651376146789]\n",
      "Average Precision:  0.9763249263109655\n",
      "Average Recall:  0.9734284260117191\n",
      "Average F1:  0.9741865883711538\n",
      "TF-ICF 30% test\n",
      "accuracy 0.9664429530201343\n",
      "confusion matrix [[125, 0, 0, 0, 0], [7, 101, 1, 0, 3], [1, 2, 108, 1, 1], [0, 0, 0, 137, 0], [2, 1, 1, 0, 105]]\n",
      "confusionMatrix [[125, 0, 0, 0, 0], [7, 101, 1, 0, 3], [1, 2, 108, 1, 1], [0, 0, 0, 137, 0], [2, 1, 1, 0, 105]]\n",
      "Precision:  [0.9259259259259259, 0.9711538461538461, 0.9818181818181818, 0.9927536231884058, 0.963302752293578]\n",
      "Recall:  [1.0, 0.9017857142857143, 0.9557522123893806, 1.0, 0.963302752293578]\n",
      "F1:  [0.9615384615384615, 0.9351851851851851, 0.968609865470852, 0.9963636363636363, 0.963302752293578]\n",
      "Average Precision:  0.9669908658759875\n",
      "Average Recall:  0.9641681357937346\n",
      "Average F1:  0.9649999801703426\n",
      "TF-ICF 40% test\n",
      "accuracy 0.9614093959731543\n",
      "confusion matrix [[122, 0, 0, 1, 0], [4, 101, 2, 0, 2], [8, 1, 108, 1, 0], [0, 0, 0, 135, 0], [1, 2, 0, 1, 107]]\n",
      "confusionMatrix [[122, 0, 0, 1, 0], [4, 101, 2, 0, 2], [8, 1, 108, 1, 0], [0, 0, 0, 135, 0], [1, 2, 0, 1, 107]]\n",
      "Precision:  [0.9037037037037037, 0.9711538461538461, 0.9818181818181818, 0.9782608695652174, 0.981651376146789]\n",
      "Recall:  [0.991869918699187, 0.926605504587156, 0.9152542372881356, 1.0, 0.963963963963964]\n",
      "F1:  [0.945736434108527, 0.9483568075117371, 0.9473684210526316, 0.989010989010989, 0.9727272727272728]\n",
      "Average Precision:  0.9633175954775476\n",
      "Average Recall:  0.9595387249076885\n",
      "Average F1:  0.9606399848822316\n",
      "TF-IDF 20% test\n",
      "accuracy 0.924496644295302\n",
      "confusion matrix [[115, 0, 2, 0, 0], [10, 102, 2, 3, 1], [6, 0, 96, 1, 0], [0, 0, 0, 130, 0], [4, 2, 10, 4, 108]]\n",
      "confusionMatrix [[115, 0, 2, 0, 0], [10, 102, 2, 3, 1], [6, 0, 96, 1, 0], [0, 0, 0, 130, 0], [4, 2, 10, 4, 108]]\n",
      "Precision:  [0.8518518518518519, 0.9807692307692307, 0.8727272727272727, 0.9420289855072463, 0.9908256880733946]\n",
      "Recall:  [0.9829059829059829, 0.864406779661017, 0.9320388349514563, 1.0, 0.84375]\n",
      "F1:  [0.9126984126984128, 0.918918918918919, 0.9014084507042254, 0.9701492537313433, 0.9113924050632912]\n",
      "Average Precision:  0.9276406057857992\n",
      "Average Recall:  0.9246203195036913\n",
      "Average F1:  0.9229134882232384\n",
      "TF-IDF 30% test\n",
      "accuracy 0.9194630872483222\n",
      "confusion matrix [[118, 0, 2, 1, 0], [12, 104, 4, 2, 3], [3, 0, 95, 2, 3], [0, 0, 0, 128, 0], [2, 0, 9, 5, 103]]\n",
      "confusionMatrix [[118, 0, 2, 1, 0], [12, 104, 4, 2, 3], [3, 0, 95, 2, 3], [0, 0, 0, 128, 0], [2, 0, 9, 5, 103]]\n",
      "Precision:  [0.8740740740740741, 1.0, 0.8636363636363636, 0.927536231884058, 0.944954128440367]\n",
      "Recall:  [0.9752066115702479, 0.832, 0.9223300970873787, 1.0, 0.865546218487395]\n",
      "F1:  [0.921875, 0.9082969432314411, 0.892018779342723, 0.9624060150375939, 0.9035087719298246]\n",
      "Average Precision:  0.9220401596069726\n",
      "Average Recall:  0.9190165854290043\n",
      "Average F1:  0.9176211019083166\n",
      "TF-IDF 40% test\n",
      "accuracy 0.9194630872483222\n",
      "confusion matrix [[114, 1, 1, 1, 0], [5, 101, 3, 3, 1], [11, 0, 96, 0, 2], [0, 0, 0, 131, 0], [5, 2, 10, 3, 106]]\n",
      "confusionMatrix [[114, 1, 1, 1, 0], [5, 101, 3, 3, 1], [11, 0, 96, 0, 2], [0, 0, 0, 131, 0], [5, 2, 10, 3, 106]]\n",
      "Precision:  [0.8444444444444444, 0.9711538461538461, 0.8727272727272727, 0.9492753623188406, 0.9724770642201835]\n",
      "Recall:  [0.9743589743589743, 0.8938053097345132, 0.8807339449541285, 1.0, 0.8412698412698413]\n",
      "F1:  [0.9047619047619048, 0.9308755760368663, 0.8767123287671234, 0.9739776951672863, 0.9021276595744682]\n",
      "Average Precision:  0.9220155979729174\n",
      "Average Recall:  0.9180336140634914\n",
      "Average F1:  0.9176910328615298\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    df = pd.read_csv('BBC News Train.csv')\n",
    "    print(df.head())\n",
    "    print()\n",
    "    print(\"TF-ICF 20% test\")\n",
    "    test_X, conditionProb,prior_prob,mapping = train(df,0.2)\n",
    "    predict(test_X, conditionProb,prior_prob,mapping)\n",
    "    print(\"TF-ICF 30% test\")\n",
    "    test_X, conditionProb,prior_prob,mapping = train(df,0.3)\n",
    "    predict(test_X, conditionProb,prior_prob,mapping)\n",
    "    print(\"TF-ICF 40% test\")\n",
    "    test_X, conditionProb,prior_prob,mapping = train(df,0.4)\n",
    "    predict(test_X, conditionProb,prior_prob,mapping)\n",
    "    print(\"TF-IDF 20% test\")\n",
    "    test_X, conditionProb,prior_prob,mapping = trainIDF(df,0.2)\n",
    "    predict(test_X, conditionProb,prior_prob,mapping)\n",
    "    print(\"TF-IDF 30% test\")\n",
    "    test_X, conditionProb,prior_prob,mapping = trainIDF(df,0.3)\n",
    "    predict(test_X, conditionProb,prior_prob,mapping)\n",
    "    print(\"TF-IDF 40% test\")\n",
    "    test_X, conditionProb,prior_prob,mapping = trainIDF(df,0.4)\n",
    "    predict(test_X, conditionProb,prior_prob,mapping)\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16e90cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
